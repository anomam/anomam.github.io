(this.webpackJsonpghio_react=this.webpackJsonpghio_react||[]).push([[0],{100:function(e,t,n){e.exports=n.p+"static/media/Paris_map.5c4ba849.png"},101:function(e,t,n){e.exports=n.p+"static/media/eltorobravo.d3f0bb4a.jpg"},102:function(e,t,n){e.exports=n.p+"static/media/pendulum.515eeb0d.gif"},103:function(e,t,n){e.exports=n.p+"static/media/handwriting.938be1e4.png"},133:function(e,t,n){e.exports=n(438)},438:function(e,t,n){"use strict";n.r(t);var a=n(0),o=n.n(a),i=n(7),r=n.n(i),l=n(491),s=n(487),c=n(490),m=n(488),u=n(489),h=n(11),p=n(117),d=n(471),g=n(440),f=n(122),y=n(473),b=n(60),w=n(474),v="\n  My name is **Marc Abou Anoma**. I'm a data science and machine learning enthusiast living in **Tokyo**. \n  My educational background is in numerical modeling and scientific computing for energy systems, but I have broad interests in machine learning, reinforcement learning, \n  and full stack application development.  \n  In my spare time I enjoy learning about new technical subjects, and above all I like to build things using what I learn. \n  So the goal here is to share these projects and thoughts with people with similar interests. \n  I may also occasionally post about non-technical subjects! You can find a list of those in the **Projects** and **Blog** sections of this website.  \n  Please don't hesitate to reach out via comments or using the social medial links.  \n  \n  Thank you for your visit!\n  ",k=Object(d.a)((function(e){return{content:{padding:e.spacing(2)}}})),x={overrides:{p:{component:g.a,props:{variant:"body2",align:"justify"}}}};var E=function(){var e=k();return o.a.createElement(y.a,{container:!0,spacing:3,direction:"column",alignItems:"center"},o.a.createElement(y.a,{item:!0,xs:12,sm:10,md:8},o.a.createElement(f.a,null,o.a.createElement("div",{className:e.content},o.a.createElement(g.a,{variant:"h4"},"Bio")),o.a.createElement(w.a,null),o.a.createElement("div",{className:e.content},o.a.createElement(b.a,{options:x},v)))))},j=n(476),_=n(477),I=n(480),$=n(479),N=n(478),P=n(475),T=Object(d.a)((function(e){return{tag:{display:"inline-block",border:"1px solid",padding:"5px",margin:"2px",color:"black",borderColor:e.palette.primary.light,borderRadius:"5px",fontSize:11},tags:{display:"block",margin:"5px 0px 5px 0px"}}}));function O(e){var t=e.tags,n=T(),a=t.map((function(e){return o.a.createElement("span",{key:e,className:n.tag},e)}));return o.a.createElement("div",{className:n.tags},o.a.createElement("div",{className:"flex flex-row flex-wrap"},a))}var S=n(21),A=function(e){var t=e.link,n=e.children,a=e.className,i=void 0===a?"":a,r=t.to.startsWith("http"),l=o.a.createElement("a",{href:t.to,className:"".concat("flex flex-row items-center"," ").concat(i)},n),s=o.a.createElement(S.b,{to:t.to,className:"".concat("flex flex-row items-center"," ").concat(i),exact:!0},n);return r?l:s};A.defaultProps={className:""};var M=A,G=Object(d.a)({card:{maxHeight:"400px"},media:{maxWidth:"100%",height:"140px"},link:{textDecoration:"none",marginRight:"3px"}}),R={overrides:{p:{component:g.a,props:{variant:"body2",align:"justify",color:"textSecondary"}}}};function D(e){var t=e.project,n=G(),a="  ".concat(t.summary),i=t.links.map((function(e){return o.a.createElement(M,{link:e,className:n.link},o.a.createElement(P.a,{variant:"contained",size:"small",color:"secondary"},e.name))}));return o.a.createElement(j.a,{className:n.card},o.a.createElement(_.a,null,o.a.createElement(N.a,{component:"img",alt:t.image.caption,className:n.media,image:t.image.src,title:t.title}),o.a.createElement($.a,null,o.a.createElement(g.a,{gutterBottom:!0,variant:"h5",component:"h2"},t.title),o.a.createElement(O,{tags:t.tags}),o.a.createElement(b.a,{options:R},a))),o.a.createElement(I.a,null,o.a.createElement("div",null,i)))}var q=n(98),B=n.n(q),C=n(99),F=n.n(C),L=n(100),z=n.n(L),H=n(101),W=n.n(H),U=n(102),Y=n.n(U),V=n(103),J=n.n(V),K=[{title:"Handwriting generation with RNNs",summary:"Combining RNNs with a mixture of gaussians to teach an algorithm how to handwrite.",description:"",tags:["deep-learning","python","pytorch"],date:new Date,image:{src:J.a,caption:"Unconditional generation"},links:[{to:"https://github.com/anomam/handwriting-pytorch",name:"Github"},{to:"blog/1616716800",name:"Blog Post"}]},{title:"Neural Style Transfer",summary:"Streamlit application for Neural Style Transfer using Keras and Tensorflow.",description:"",tags:["deep-learning","python","tensorflow"],date:new Date,image:{src:"https://github.com/anomam/nst/raw/master/outputs/example/starry_tokyo_tower.gif",caption:"NST"},links:[{to:"https://github.com/anomam/nst",name:"Github"},{to:"blog/1585353600",name:"Blog Post"}]},{title:"Pendulum RL",summary:"Reinforcement Learning for pendulum equilibrium using numpy and value function iteration.",description:"",tags:["reinforcement learning","numpy"],date:new Date,image:{src:Y.a,caption:"Pendulum RL"},links:[{to:"blog/1543622400",name:"Blog Post"}]},{title:"pvfactors",summary:"Irradiance modeling made simple: open-source Python model for bifacial irradiance modeling, published at IEEE PVSC 44 (2017).",description:"\n        This is a Python package I developed at SunPower that originated from a mathematical model we validated experimentally, and which calculates bifacial irradiance and diffuse shading on photovoltaic (PV) arrays. I gave an oral presentation of this model at the IEEE PVSC 44 conference.\npvfactors makes that model open-source on Github, but the package has turned into something closer to a framework now, where users can plug in their own irradiance models, view factor calculations and even geometries.\npvfactors is also now used in the industry-standard and much larger open-source photovoltaic modeling package called pvlib (as of pvlib v0.6.1).\n        ",tags:["python","R&D","linear algebra"],date:new Date,image:{src:B.a,caption:"pvfactors"},links:[{to:"https://sunpower.github.io/pvfactors/",name:"Github"},{to:"https://pdfs.semanticscholar.org/ebb2/35e3c3796b158e1a3c45b40954e60d876ea9.pdf",name:"Paper"}]},{title:"Passive radiative cooling",summary:"Nature paper on first-time in the world daytime passive radiative cooling using a nanophotonics device.",description:"",tags:["energy","renewable","R&D"],date:new Date,image:{src:F.a,caption:"Passive radiative cooling"},links:[{to:"https://www.nature.com/articles/nature13883",name:"Paper"}]},{title:"Paris Real Estate Map",summary:"Interactive map on Paris real estate prices using DVF data.",description:"",tags:["angularjs","python","real-estate"],date:new Date,image:{src:z.a,caption:"Paris real estate"},links:[{to:"https://www.marcanoma.com/dvf/index",name:"Demo"}]},{title:"El Toro Bravo",summary:"Fun mechatronics project where we built an autonomous robot for sumo battling during grad school at Stanford.",description:"",tags:["mechatronics","grad school","stanford"],date:new Date,image:{src:W.a,caption:"El Toro Bravo"},links:[{to:"https://me210team10torobravo.weebly.com/",name:"Website"},{to:"https://news.stanford.edu/news/2013/march/student-robot-duel-032013.html",name:"Video"}]}];var X=function(){return o.a.createElement(y.a,{container:!0,spacing:3},K.map((function(e){return o.a.createElement(y.a,{item:!0,key:e.title,xs:12,sm:6,md:3},o.a.createElement(D,{project:e}))})))};var Q=function(){var e=Object(h.f)();return o.a.createElement("div",{style:{margin:"20px"}},o.a.createElement("h3",null,"No match found for",o.a.createElement("code",null,e.pathname)))},Z=n(53),ee=n(104),te=n.n(ee),ne=n(105),ae=n.n(ne),oe=(n(423),n(106)),ie=n.n(oe),re=n(77),le=(n(436),function(e){return o.a.createElement(ae.a,function(e){return Object(Z.a)(Object(Z.a)({},e),{},{escapeHtml:!1,plugins:[ie.a],renderers:Object(Z.a)(Object(Z.a)({},e.renderers),{},{math:function(e){var t=e.value;return o.a.createElement(re.BlockMath,null,t)},inlineMath:function(e){var t=e.value;return o.a.createElement(re.InlineMath,null,t)}})})}(e))});function se(e){var t=e.children,n=Object(a.useRef)(null);return Object(a.useEffect)((function(){n.current&&n.current.querySelectorAll("pre code").forEach((function(e){te.a.highlightBlock(e)}))}),[t]),o.a.createElement("div",{ref:n},o.a.createElement(le,null,t))}var ce=n(52),me=Object(d.a)((function(e){return{content:{padding:e.spacing(3)},grid:{width:"100%"}}}));function ue(){var e=me(),t=Object(h.g)().blogPostId;if(!Object.prototype.hasOwnProperty.call(ce,t))return o.a.createElement(Q,null);var n=ce[t],a="# ".concat(n.title,"\n#### ").concat(n.date.toString()),i="".concat(a,"\n\n").concat(n.content);return o.a.createElement(y.a,{container:!0},o.a.createElement(y.a,{item:!0,sm:!1,md:1,lg:2}),o.a.createElement(y.a,{item:!0,sm:12,md:10,lg:8},o.a.createElement(f.a,{elevation:3,className:e.content},o.a.createElement(se,null,i),o.a.createElement(O,{tags:n.tags}))),o.a.createElement(y.a,{item:!0,sm:!1,md:1,lg:2}))}function he(e){var t=e.post;return o.a.createElement(y.a,{item:!0,xs:12},o.a.createElement(_.a,{component:S.a,to:"blog/".concat(t.id)},o.a.createElement(j.a,null,o.a.createElement("div",null,o.a.createElement($.a,null,o.a.createElement(g.a,{component:"h2",variant:"h5"},t.title),o.a.createElement(g.a,{variant:"subtitle1",color:"textSecondary"},t.date),o.a.createElement(O,{tags:t.tags}),o.a.createElement(g.a,{variant:"subtitle1",paragraph:!0},t.summary),o.a.createElement(g.a,{variant:"subtitle1",color:"secondary"},"Continue reading..."))))))}function pe(){var e,t=(e=ce,Object.keys(e).map((function(t){return e[t]})).sort((function(e,t){return e.id<t.id?1:-1})));return o.a.createElement(y.a,{container:!0,spacing:4},t.map((function(e){return o.a.createElement(he,{key:e.id,post:e})})))}function de(){return o.a.createElement(h.c,null,o.a.createElement(h.a,{path:"/",exact:!0,component:E}),o.a.createElement(h.a,{path:"/projects",exact:!0,component:X}),o.a.createElement(h.a,{path:"/blog",exact:!0,component:pe}),o.a.createElement(h.a,{path:"/blog/:blogPostId",exact:!0,component:ue}),o.a.createElement(h.a,{path:"*"},o.a.createElement(Q,null)))}var ge,fe,ye=n(486),be=n(492),we=n(118),ve=n(18),ke=n(484),xe=n(485),Ee=n(493),je=n(496),_e=n(114),Ie=n.n(_e),$e=n(482),Ne=n(111),Pe=n.n(Ne),Te=n(112),Oe=n.n(Te),Se=n(113),Ae=n.n(Se),Me=n(494),Ge=n(116),Re=n.n(Ge),De=n(483),qe=[{type:"LinkedIn",url:"https://www.linkedin.com/in/marc-anoma-20b74940/"},{type:"Twitter",url:"https://twitter.com/anomamarc"},{type:"Google Scholar",url:"https://scholar.google.co.jp/citations?user=RrWWCvQAAAAJ&hl=en&oi=sra"},{type:"GitHub",url:"https://github.com/anomam"}],Be=n(109),Ce=n.n(Be),Fe=n(108),Le=n.n(Fe),ze=n(110),He=n.n(ze),We=n(107),Ue=n.n(We),Ye="github",Ve="linkedin",Je="twitter",Ke="scholar",Xe=(ge={},Object(ve.a)(ge,Ye,{component:Ue.a}),Object(ve.a)(ge,Ve,{component:Le.a}),Object(ve.a)(ge,Je,{component:Ce.a}),Object(ve.a)(ge,Ke,{component:He.a}),ge),Qe="GitHub",Ze="LinkedIn",et="Twitter",tt="Google Scholar",nt=(fe={},Object(ve.a)(fe,Qe,Ye),Object(ve.a)(fe,Ze,Ve),Object(ve.a)(fe,et,Je),Object(ve.a)(fe,tt,Ke),fe),at={color:"inherit"};function ot(e){var t=e.type,n=e.color,a=void 0===n?at.color:n;if(!Object.prototype.hasOwnProperty.call(nt,t))return t;var i=Xe[nt[t]];return o.a.createElement(i.component,{color:a})}ot.defaultProps=at;var it=ot,rt=Object(d.a)((function(e){return{root:{flexGrow:1},usrIcon:{marginRight:e.spacing(2)},link:{"&:hover":{textDecoration:"none"},color:"inherit"},title:{flexGrow:1},sectionDesktop:Object(ve.a)({display:"none"},e.breakpoints.up("sm"),{display:"flex"}),sectionMobile:Object(ve.a)({display:"flex"},e.breakpoints.up("sm"),{display:"none"})}})),lt=[{name:"Home",link:"/",Icon:Pe.a},{name:"Projects",link:"/projects",Icon:Oe.a},{name:"Blog",link:"/blog",Icon:Ae.a}];function st(){var e=rt(),t=o.a.useState(null),n=Object(we.a)(t,2),a=n[0],i=n[1],r=Boolean(a),l=o.a.createElement(Ee.a,{anchorEl:a,anchorOrigin:{vertical:"top",horizontal:"right"},id:"menu-mobile",keepMounted:!0,transformOrigin:{vertical:"top",horizontal:"right"},open:r,onClose:function(){i(null)}},lt.map((function(e){return o.a.createElement(je.a,{key:e.name,component:S.a,to:e.link,replace:!0},o.a.createElement($e.a,{color:"inherit"},o.a.createElement(e.Icon,{color:"primary"})),e.name)})),qe.map((function(t){return o.a.createElement(je.a,{key:t.type,component:De.a,href:t.url,className:e.link},o.a.createElement($e.a,{color:"inherit"},o.a.createElement(it,{type:t.type,color:"primary"})),t.type)})));return o.a.createElement("div",{className:e.root},o.a.createElement(ke.a,{position:"static"},o.a.createElement(xe.a,null,o.a.createElement(Ie.a,{edge:"start",className:e.usrIcon,color:"inherit","aria-label":"menu"}),o.a.createElement(g.a,{variant:"h6",className:e.title},"anomam.github.io"),o.a.createElement("div",{className:e.sectionDesktop},lt.map((function(t){return o.a.createElement(Me.a,{key:t.name,title:t.name},o.a.createElement(S.a,{to:t.link,className:e.link,replace:!0},o.a.createElement($e.a,{color:"inherit"},o.a.createElement(t.Icon,null))))})),qe.map((function(e){return o.a.createElement(Me.a,{key:e.type,title:e.type},o.a.createElement($e.a,{color:"inherit",href:e.url},o.a.createElement(it,{type:e.type})))}))),o.a.createElement("div",{className:e.sectionMobile},o.a.createElement($e.a,{"aria-label":"show more","aria-controls":"menu-mobile","aria-haspopup":"true",onClick:function(e){i(e.currentTarget)},color:"inherit"},o.a.createElement(Re.a,null))))),l)}function ct(){return o.a.createElement(g.a,{variant:"body2",color:"textSecondary",align:"center"},"Copyright \xa9 ",o.a.createElement(De.a,{color:"inherit",href:"https://anomam.github.io/"},"anomam.github.io")," ",(new Date).getFullYear(),".")}var mt=Object(d.a)((function(e){return{container:{paddingTop:e.spacing(4)},footer:{marginBottom:"20px"}}})),ut=function(e){var t=e.children,n=mt();return o.a.createElement("main",null,o.a.createElement("header",null,o.a.createElement(st,null)),o.a.createElement("section",null,o.a.createElement(ye.a,{className:n.container},t)),o.a.createElement("footer",{className:n.footer},o.a.createElement(be.a,{pt:4},o.a.createElement(ct,null))))},ht=Object(p.a)();function pt(){return o.a.createElement(h.b,{history:ht},o.a.createElement(ut,null,o.a.createElement(de,null)))}ht.listen((function(e){!function(e){window.gtag&&window.gtag("config","UA-133656858-2",{page_path:"".concat(e.pathname).concat(e.search).concat(e.hash)})}(e)}));var dt=Object(s.a)({palette:{primary:{main:m.a[600]},secondary:{main:u.a[700]}}});r.a.render(o.a.createElement(o.a.StrictMode,null,o.a.createElement(c.a,{theme:dt},o.a.createElement(l.a,null),o.a.createElement(pt,null))),document.getElementById("root"))},52:function(e){e.exports=JSON.parse('{"1543622400":{"id":1543622400,"title":"Inverted Pendulum RL with numpy","author":"Marc Anoma","date":"2018-12-01","summary":"Using world-model estimation and value function iteration to solve a finite state inverted pendulum problem","tags":["numpy","reinforcement learning","value function iteration"],"content":"\\nThis was inspired from a Stanford CS229 2018 problem set. Using an already defined finite-state inverted pendulum model:\\n\\n- I used numpy to estimate the world model (transition probabilities and rewards)\\n- I applied value function iteration to estimate the optimal value function given the world model estimates\\n- finally, I took the greedy policy wrt to the obtained value function in order to create the RL agent\\n\\nAfter a number of iterations I was able to get to the following results.\\n\\n![inverter pendulum](https://raw.githubusercontent.com/anomam/anomam.github.io/master/static/media/pendulum.515eeb0d.gif)\\n"},"1585353600":{"id":1585353600,"title":"Neural Style Transfer with Tensorflow","author":"Marc Anoma","date":"2020-03-28","summary":"My notes on building a Streamlit application for Neural Style Transfer with Keras and Tensorflow","tags":["python","deep-learning","tensorflow","streamlit"],"content":"\\n<img alt=\\"nst-example\\" src=\\"https://github.com/anomam/nst/raw/master/outputs/example/starry_tokyo_tower.gif\\" style=\\"display:block; margin-left:auto; margin-right:auto; max-width: 100%\\"/>\\n\\n# Introduction\\n\\nThis is one of those classic but awesome deep-learning projects that most DL practionners implement at some point or another because of its relative simplicity and the fun visual outputs obtained as results.\\n\\nI had a few purposes when I started working on this project:\\n\\n- I wanted to get a better handle of Keras and Tensorflow\\n- I wanted to give a try to [Streamlit](https://streamlit.io/): which is a Python library to create quick interactive apps\\n- I wanted to create fun pictures to decorate my house :)\\n\\nThe paper describing the Neural Style Transfer technique was written by [Gatys et al. in 2015](https://arxiv.org/abs/1508.06576).\\n\\n# Content\\n\\nWhat I found fascinating the first time I read this paper was applying back-propagation not to udpate the weights of the neural architecture, but to update the input of the data.\\n\\nOf course, a crucial trick is that the network has to be pretrained on a lot of images, and the author of the paper also had great inspiration on what type of loss function to use.\\n\\nIt is quite clever:\\n\\n$$\\n\\\\mathcal{L}_{total}(\\\\vec{p}, \\\\vec{a}, \\\\vec{x}) = \\\\alpha\\\\mathcal{L}_{content}(\\\\vec{p}, \\\\vec{x}) + \\\\beta\\\\mathcal{L}_{style}(\\\\vec{a}, \\\\vec{x})\\n$$\\n\\nwhere $\\\\vec{p}$ is the photograph, $\\\\vec{a}$ is the artwork, and $\\\\vec{x}$ is the input.\\n\\nBoth loss terms are square root type losses.\\nThe `content` part minimizes the encoded feature representation for both the photograph and the input data at a certain depth of the network:\\n\\n$$\\n\\\\mathcal{L}_{content}(\\\\vec{p}, \\\\vec{a}, l) = \\\\frac{1}{2}\\\\sum_{i,j} (F^{l}_{ij} - P^{l}_{ij})^{2}\\n$$\\n\\nFor my implementation I used `block4_conv2`.\\n\\nThe `style` part minimizes the square error between the style matrix of the artwork and the input data, for a layer l:\\n\\n$$\\nE_{l} = \\\\frac{1}{4N^2_{l}M^2_{l}}\\\\sum_{i,j} (G^{l}_{ij} - A^{l}_{ij})^{2}\\n$$\\n\\nThen\\n\\n$$\\n\\\\mathcal{L}_{style}(\\\\vec{a}, \\\\vec{x}) = \\\\sum_{l}w_{l}E_{l}\\n$$\\n\\nWhere the style matrix is simply the dot product of the encoded feature representation matrix with its transpose, at a given layer l:\\n\\n$$\\nG^l_{ij} = \\\\sum_{k}F^l_{ik}F^l_{kj}\\n$$\\n\\nOr\\n\\n$$\\nG^l = A^l.(A^l)^t\\n$$\\n\\nSo in the end there are a lot of hyperparameters and weights that can be tuned manually to try to lead to better results.\\n\\nWhich is why I thought it would be nice to have a UI to do it, hence Streamlit!\\n\\n# Challenges\\n\\nThe hardest part for me by far was to get a good working installation of tensorflow to work with my GPU. At that time Tensorflow 2 was still pretty new, so maybe it\'s better with that new major version now, but somehow it was a bit of a struggle for me with v1.8 (as opposed to Pytorch which was a breeze to install for my other projects).\\nIn order to solve this and to make my setup easily repeatable I ended up using the Keras Dockerfile examples from their repo, but that was a complexity I did not expect before starting.\\n\\nI remember also struggling a tiny bit to re-arrange the network to add the variable input at the entrance and replacing the Max Pool layers with Avg Pool layers as suggested by other projects.\\n\\n# Results and final notes\\n\\nThe workflow in the Streamlit app looked like this in the end:\\n\\n<img alt=\\"nst-streamlit\\" src=\\"https://github.com/anomam/nst/raw/master/outputs/example/NST_app_demo.gif\\" style=\\"display:block; margin-left:auto; margin-right:auto; max-width: 100%\\"/>\\n\\nAnd I also used imgflip.com to create the gif shown in this post.\\n\\nI hope you\'ll find this project useful if you decide to try Neural Style Transfer yourself. You can find the repo [here](https://github.com/anomam/neural-style-transfer-tensorflow).\\n"},"1593561600":{"id":1593561600,"title":"Python argmax","author":"Marc Anoma","date":"2020-07-01","summary":"Native `argmax()` function in Python without numpy or Tensorflow","tags":["python","maths"],"content":"\\nThere\'s no native `argmax` function in Python... and installing numpy or Tensorflow just for this would be like hunting snails with a rifle.\\n\\nHere\'s one option:\\n\\n```python\\ndef argmax(x: list) -> int: return max(range(len(x)), key=lambda i: x[i])\\n```\\n\\nYou can also define `argmin` in a similar fashion.\\n"},"1594771200":{"id":1594771200,"title":"I love FastAPI","author":"Marc Anoma","date":"2020-07-15","summary":"A small summary of all the reasons why I love FastAPI","tags":["python","backend"],"content":"\\nI love [FastAPI](https://fastapi.tiangolo.com/) for multiple reasons\\n\\n1. the documentation is pure gold: it teaches about not only fastapi, but web development in general. And you can tell that [Tiangolo](https://github.com/tiangolo/) knows what he\'s talking about\\n2. it\'s lightweight\\n3. types are strongly enforced on all views: and in the process I\'m learning about how powerful [Pydantic](https://pydantic-docs.helpmanual.io/) is\\n4. it auto-documents and creates UI with forms to test all endpoints (~swagger)\\n5. it uses elegant dependency injection and makes things like OAuth2 relatively painless\\n6. python asyncio is native\\n7. the repo has 100% test coverage...\\n\\nFor me, it\'s one of those modern packages that I\'ll definitely use as inspiration in future work.\\nHopefully the Python community will pick it up and make sure the project has a long life.\\n"},"1594857600":{"id":1594857600,"title":"Understanding try/except/else/finally once and for all","author":"Marc Anoma","date":"2020-07-16","summary":"try/except is quite intuitive, but I always had to search again the else/finally clauses whenever I would stumble on it. Here is a simple example from the Python official documentation to put it to rest","tags":["python"],"content":"\\nFrom the [Python 3 documentation](https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions):\\n\\n```python\\n>>> def divide(x, y):\\n...     try:\\n...         result = x / y\\n...     except ZeroDivisionError:\\n...         print(\\"division by zero!\\")\\n...     else:\\n...         print(\\"result is\\", result)\\n...     finally:\\n...         print(\\"executing finally clause\\")\\n...\\n>>> divide(2, 1)\\nresult is 2.0\\nexecuting finally clause\\n>>> divide(2, 0)\\ndivision by zero!\\nexecuting finally clause\\n>>> divide(\\"2\\", \\"1\\")\\nexecuting finally clause\\nTraceback (most recent call last):\\n  File \\"<stdin>\\", line 1, in <module>\\n  File \\"<stdin>\\", line 3, in divide\\nTypeError: unsupported operand type(s) for /: \'str\' and \'str\'\\n```\\n\\nIn conclusion:\\n\\n- `finally` is always executed\\n- `else` is executed only if no exception is raised\\n"},"1616716800":{"id":1616716800,"title":"Handwriting generation with RNNs (unconditional)","author":"Marc Anoma","date":"2021-03-26","summary":"Teaching a neural network how to write using an RNN + Mixture of Gaussian architecture","tags":["python","backend","deep-learning","pytorch"],"content":"\\n<img alt=\\"handwriting\\" src=\\"https://raw.githubusercontent.com/anomam/handwriting-pytorch/main/log/plots/example_generated_01.png\\" style=\\"display:block; margin-left:auto; margin-right:auto; max-width: 100%\\"/>\\n\\n# Introduction\\n\\nIt was quite an exciting project for me to reimplement part of the awesome [handwriting generation paper](https://arxiv.org/abs/1308.0850) from Alex Graves (2015). I picked it because I wanted to learn a new (for me) application of RNNs that gave really fun results, and it was also my first bigger-size project with Pytorch, which was quite enjoyable overall.\\n\\nI liked Pytorch easy installation process with my GPU and `pip`, using the Pytorch [decision matrix](https://pytorch.org/get-started/locally/#start-locally). Besides, the interface and the dynamic nature of the NN graphs made it a very familiar environment for me since it just felt like coding with `numpy`, which I know quite well through my scientific computing work (e.g. in [pvfactors](https://github.com/SunPower/pvfactors)).\\n\\nAn important note also is that I only focused on the unconditional generation part, but I\'ll update my code with the conditional generation section as soon as I can!\\n\\n# Content\\n\\nThe most exciting part for me reading this paper was the idea of using not just a neural network, but a recurrent neural network in order to estimate the parameters of a mixture of gaussians.\\nI found this quite powerful, because while neural networks with sum of square loss functions end up estimating the expected value of a response conditioned on features $y|X$:\\n\\n$$\\n\\\\mathbb{E}[y|X]\\n$$\\n\\nIn this paper the idea was to estimate the whole distribution of $y|X$ like presented in the [Mixture Density Networks paper](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf) from Bishop et al (1994), while keeping track of history.\\n\\nAlso, since the idea of the paper is to do generation of handwriting offsets, there\'s no concept of label $y$ here. We\'re modeling $x_t$ at different timesteps $t$ instead.\\n\\nSuch that roughly speaking, we\'re modeling:\\n\\n$$\\nP[x_{t+1} | x_t, h_t] = \\\\sum_j\\\\pi_{j,t}\\\\mathcal{N}(x_{t+1}|\\\\mu_{j,t}, \\\\sigma_{j,t}, \\\\rho_{j,t})\\n$$\\n\\nwhere we estimate $\\\\mu_{j,t}, \\\\sigma_{j,t}, \\\\rho_{j,t}$ and $\\\\pi_{j,t}$ using the recurrent neural network, the value $x_t$ as input, but also the encoded past history saved as a \\"hidden state\\" in the RNNs, $h_t$.\\n\\nIt is important to note that the $\\\\pi_{j,t}$ terms do not represent weights, but rather the probability that a Gaussian with index $j$ within the Mixture of Gaussians is selected for the given timestep $t$. So they form a multinomial distribution at a given timestep $t$.\\n\\nFor handwriting generation, the additional parameter we\'re modeling is whether or not the next pen offset is lifted or not (when the pen is down it is drawing, otherwise it just moves to a different location on the canvas).\\nThis can be simply modeled by a bernouilli distribution with probability $\\\\phi$, such that (assuming conditional independence between the next offset value and the lift of the pen):\\n\\n$$\\n\\\\begin{aligned}\\nP[x_{t+1} | x_t, h_t] = (\\\\sum_j\\\\pi_{j,t}\\\\mathcal{N}((x_{t+1})_{1,2}|\\\\mu_{j,t}, \\\\sigma_{j,t}, \\\\rho_{j,t})) \\\\\\\\\\n* \\\\phi_t^{(x_{t+1})_3}(1 - \\\\phi_t)^{1 - (x_{t+1})_3}\\n\\\\end{aligned}\\n$$\\n\\nIn the end, the final goal during training is to simply minimize the negative log likelihood from this probability function, summed over all the timesteps in a writing sequence (or a strokeset):\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{L}(parameters) =\\n-\\\\sum_{timesteps}\\\\{log(\\\\sum_j\\\\pi_{j,t}\\\\mathcal{N}((x_{t+1})_{1,2}|\\\\mu_{j,t}, \\\\sigma_{j,t}, \\\\rho_{j,t})) \\\\\\\\\\n+ ((x_{t+1})_3.log\\\\phi_t + (1 - (x_{t+1})_3).log(1 - \\\\phi_{t}))\\\\}\\n\\\\end{aligned}\\n$$\\n\\nYou\'ve probably noticed the cross-entropy loss term added to the loss function and used for general binary classification tasks.\\n\\n# Challenges\\n\\nI actually struggled quite a bit before I was able to obtain good looking results. What unblocked me was a concept actually described in the paper, that I thought was minor at first, but which ended up making a big difference. It\'s the idea of adding a \\"bias\\" term during generation.\\nThe concept is quite simple, by adding a bias term at the right place in the equations for generating data, it is possible to generate offsets the are closer to the mean of the calculated distributions. This allows to avoid unlikely but very detrimental offset occurences when sampling from the mixture of gaussians.\\n\\nI also learned a lot by reading other projects on how to transform the data to make it look more uniform and cleaner; for instance on how to align all the writing sequences so that it\'s not slanted anymore. And I also got a lot of insight on how to create and handle batches of handwriting sequences for training and generation.\\n\\n# Results and final notes\\n\\nHere are some of my results:\\n\\n- without any alignment correction on the training data\\n\\n<img alt=\\"handwriting-no-align\\" src=\\"https://raw.githubusercontent.com/anomam/handwriting-pytorch/main/log/plots/example_generated_no_align.png\\" style=\\"display:block; margin-left:auto; margin-right:auto; max-width: 100%\\"/>\\n\\n- after applying an alignment transformation on the training data:\\n\\n<img alt=\\"handwriting\\" src=\\"https://raw.githubusercontent.com/anomam/handwriting-pytorch/main/log/plots/example_generated_01.png\\" style=\\"display:block; margin-left:auto; margin-right:auto; max-width: 100%\\"/>\\n\\nI hope you find this work useful or at least interesting! You can find the code [on Github](https://github.com/anomam/handwriting-pytorch).\\n"}}')},98:function(e,t,n){e.exports=n.p+"static/media/bifacial.dc78e8f4.jpg"},99:function(e,t,n){e.exports=n.p+"static/media/passive_rad_cooling.69cdb4d4.jpg"}},[[133,1,2]]]);
//# sourceMappingURL=main.aad192fd.chunk.js.map